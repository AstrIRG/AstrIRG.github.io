<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>AstrIRG</title>
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css">
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js"></script>
<link rel = "stylesheet" type = "text/css" href = "/mystyle.css" />
<link href="https://fonts.googleapis.com/css?family=Saira" rel="stylesheet">
</head>


<!--FOR SEO-->
	<meta property="og:description" content="AstrIRG: Astroinformatics Research Group">
    <meta name="author" content="AstrIRG" />
    <meta name="copyright" content="AstrIRG, Copyright (c) 2017" />
    <meta name="keywords" content="AstrIRG, Astroinformatics Research Group, IEEE Computer Society, CompSoc, IEEE CompSoc,
    Snehanshu Saha, Suryoday Basak, Margarita Safonova, Jayany Murthy">
    <meta name="description" content="AstrIRG: Astroinformatics Research Group">

    <link rel="canonical" href="http://astrirg.org" />
    <meta property="og:title" name="twitter:title" content="AstrIRG"/>
    <meta property="og:site_name" content="AstrIRG"/>
    <meta property="og:type" content="website"/>
    <meta property="og:url" content="http://astrirg" />
    <meta property="og:description" name="twitter:description" content="AstrIRG: Astroinformatics Research Group"/>
    <meta property="og:locale" content="en_IN" />
    <meta property="og:locale:alternate" content="en_US" />

    <meta name="twitter:card" content="summary" />
    <meta name="twitter:creator" content="@suryoday_basak" />

    <script type="application/ld+json">
        {
                "@context": "http://schema.org",
                "@type": "Person",
                "name": "AstrIRG",
                "jobTitle": "Research Group",
                "url": "http://astrirg.org",
		}
    </script>
    <script type="application/ld+json">
        {
            "@context": "http://schema.org",
            "@type": "WebSite",
            "url": "http://astrirg.org",
            "name": "AstrIRG",
            "author": {
                "@type": "Group",
                "name": "Astroinformatics Research Group"
            }
        }
    </script>
<!--END FOR SEO-->


<body>
<!-- This is my code lol -->
<!--<div class="jumbotron bg-image">-->
<div class="jumbotron">
  <div class="container">
  <h1 class ="jumbotron-text">A Chat With Robert Haralick on<br>the Emergence of Deep Learning</h1>
</div>
<!--   /container  -->
</div>


<nav id="myNavbar" class="navbar navbar-default navbar-inverse navbar-fixed-top" role="navigation" data-spy="affix" data-offset-top="100" >
    <!-- Brand and toggle get grouped for better mobile display -->
    <div class="container">
        <div class="navbar-header">
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#navbarCollapse">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <!--<a class="navbar-brand" href="index.html"><img class="hidden-xs" src="design_assets/logo_light.png" height="150%"></a>-->
            <a class="navbar-brand" href="/index.html">AstrIRG</a>
        </div>
        <!-- Collect the nav links, forms, and other content for toggling -->
        <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="nav navbar-nav navbar-right">
                <!--<li class="active"><a href="https://www.tutorialrepublic.com" target="_blank">Home</a></li>-->
                <li><a href="/projects.html">Projects</a></li>
                <li><a href="/people.html">People</a></li>
                <li><a href="/contact.html">Contact Us</a></li>
            </ul>
        </div>
    </div>
</nav>


<div class="container" align="justify">
	<p>
    Dr. Robert Haralick is a leader in the field of computer vision. He has
    pioneered methods and paradigms which break problems down into smaller
    parts such as those of feature extraction and subspace classification
    of data. He was a keynote speaker at the International Conference on
    Advances in Pattern Recognition (ICAPR), 2017 held in the Indian Statistical
    Institute, Bangalore and our junior researcher Suryoday Basak was able to
    catch up with him and ask him a few questions on the emergence of deep
    learning and how emerging problems should be solved with the use of machine learning.
<br><br>
<b><font color="blue">Suryoday Basak</font></b>: You have extensively contributed to image processing and a large
portion of your work has been on smart feature engineering and extraction.
This approach seems to be in contrast with methods of deep learning which are
slowly becoming more popular by the day, where the features are automatically
detected and the details often lost or often not looked at. What advantages or
disadvantages do deep learning methods offer over more traditional machine learning methods?
<br><br>
<b><font color="red">Robert Haralick</font></b>: I guess you’ve already stated one of the advantages – it's that you can
look at the last layer of a neural network and you can get some features out, so
that’s an advantage. However, the calculations that you have to go through, from
the beginning to that layer are a large, extensive set of calculations and in some
sense, you don’t know what those calculations mean. Well, you can find what the
feature means by your own pattern recognition system where you start to look at
images and what the values in the images are so that you can observe some kind
of correspondence. The techniques which start out from the features’ side and
develop the feature, for example, like mathematical morphology features - these
kinds of features where you do morphological transforms on an image, such as
opening, or closing, or dilation, or erosion, or cascading of such operations –
here the number of calculations is very small and it is based on the essence of
shape. So the results that you get inherently have to do with shape because
mathematical morphology is the study of shape. A deep learning method may come
up with something at the end that is related to a mathematical morphology feature
extractor but after it does so, you don’t know that it was a mathematical morphology
feature extractor! So I would not make the generalization that the whole world is
going to become deep learning! This certainly won’t throw deep learning away.
It has a place and it has a value, but we must remember that understanding comes
from being able to do something in the simplest way in which it can be done.
That’s the principle of Occam’s razor and deep learning doesn’t have that.
Feature extraction as it has been going on in image analysis for fifty years –
every one of those methods has something that tells you why it actually works and
what it means. I wouldn’t throw that away, and I don’t think that deep learning
actually replaces that.
<br><br>
<b><font color="blue">Suryoday Basak</font></b>: My next question is regarding something on your website that caught my
attention. You have mentioned that one of your research interests is to further
the ‘science of computer vision’. Could you please elaborate on what you mean by
'science of computer vision'?
<br><br>
<b><font color="red">Robert Haralick</font></b>: Yeah, that goal is actually a goal that I’ve had since the 1980’s and
you have to understand the context of that. When people first began to work on
computer vision, there was a whole group of that was doing it by demonstration.
They would take a dataset, often small, and would fine-tune their method for that
dataset, but the method was very brittle and it did not work when you extended
it out. And also in the 1980’s, when I said – and I said this multiple times in
various workshops and conferences where the people from Stanford, Carnegie Mellon,
Maryland, the Californian universities and all other places  would challenge the
statement that I made and the statement I made was <i>that computer vision has
to be posed as an optimization problem</i>; if you haven’t gotten to the place
where you’re posing it as an optimization problem then you don’t fully understand
what it is that you’re doing. Now, from this point of view, we can go back to the
earlier question on deep learning. Deep learning is posed as an optimization problem,
but the problem is that it’s too complex. So what we’re looking for really is
the simplest optimization problem that you can pose that gets you the required
results. The ‘science’ part comes from the fact that you did it as an optimization
problem. In the 1980’s, I said this multiple times. So let me tell you what the
researchers from MIT or Stanford said to me. They said, “Haralick, you’re a dreamer.”
That’s what they said, and I knew that all the things they were talking about
were a fad. And that fad lasted from 1982 to 1985. And then there was a new fad – 1985 to 1987.
And, well, one fad replaced another fad, and then that replaced another fad,
and it was all a fad because it was never posed as an optimization problem.
<br><br>
I had a paper that was published in that same period of time, probably in the
middle to late 1980’s*. It was called ‘Computer Vision Theory: The Lack Thereof’.
It was a widely referenced paper where I tried to refine this business that you
had to make computer vision into an optimization problem. I even mentioned what
kind of an optimization problem you have to make it into. That is what I regarded
as the science – when you get to the place where you can define the optimization
problem and solve it. That’s when you’ve accomplished the <i>science</i>.
<br><br>
<b><font color="blue">Suryoday Basak</font></b>: I'm just trying to imagine how you’ve been from one stage to another.
I suppose that it would be appropriately stated as an emotional journey, where
people are calling you a dreamer, as well as something technical, where you
fortified whatever you believed in with the use of mathematics. I find it quite
overwhelming! Here is my last question for you and this is with regard to
something for which a lot of people have been facing criticism of late. Often,
a problem is looked at in a context-specific manner and solved using feature
extraction and traditional machine learning. What has come to light of late is
that if authors are not using the more edgy methods, their papers are less likely
to get accepted. The problems, however, are of a different flavor, wherein the size
of the data is of the order of 10s of megabytes, but there are inherent complexities
and limitations of the data. What are your thoughts on using deep learning to solve
such problems, or to solve problems in general, in a very non-context-specific sense
and in small datasets?
<br><br>
<b><font color="red">Robert Haralick</font></b>: I guess there’s some part of that question that needs to be answered first.
I mentioned that what actually happens in the research area is that you get a sequence
of fads and the fads last many years, and after that, there’s a new fad. And what
happens is that different researchers jump on the bandwagon or that jump on that
train of the fad. Concurrently, if you’re doing something that’s not relative to
what’s happening in the fad, then they’re going to downgrade you saying that you’re
not doing good research. There’s a saying in the history of science which is very
like this: it’s from a book that was published by Thomas Kuhn in the 1960s.
The title of the book is ‘The Structure of Scientific Revolutions’. He didn’t call
it ‘fad’, he called it a ‘paradigm’. What he shows is that there’s always a ‘current
paradigm’ and everybody locks on to the current paradigm. But the current paradigm
doesn’t solve all of the problems, it doesn’t match all of the observations – we’re
talking about physics – and they start to look at the anomalies. Then they realize,
“oh, there’s something wrong with the 'current paradigm' and then there’s a ‘new paradigm’.
In the beginning, Thomas Kuhn says that all the people that are using the current
paradigm even make personal attacks on the scientists saying that they need to
diversify. Ultimately, when enough people work into a newer paradigm that satisfies
all of the requirements of all the data that was worked on earlier plus some new data,
they constitute the next fad, which becomes the ‘new paradigm’ and everybody starts to
join in on to the train of that paradigm and the cycle just repeats again, and again,
and again. It’s unfortunate that researchers don’t realize that this is the bigger picture.
</p>
</div>

<div class="col-sm-12">
  <p align="center">
    <img src="/images/haralick-suryo.jpeg
" class="img-responsive" align="center" width =500>
    Dr. Robert Haralick with Suryoday Basak
  </p>
  </div>

<div>
  <p>
    <hr>
*Robert Haralick, 1986, Computer vision theory: The lack thereof, <a href="https://doi.org/10.1016/0734-189X(86)90082-4">https://doi.org/10.1016/0734-189X(86)90082-4</a>
</p>
</div>

</body>
</html>
